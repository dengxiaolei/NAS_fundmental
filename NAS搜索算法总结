## 1.经典算法proxyLessNAS
传统NAS搜索算法的缺点：
   1. NAS搜索阶段只能在小型数据集上进行搜索，而在大型数据集上出现GPU消耗过大以及耗时长的缺点；
   2. 小数据集上搜索的网络模型由于数据量的限制，往往模型不是最优的；
   3. 强化学习和进化算法耗时长；
   
因此，作者提出了proxyLessNAS算法 ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware
使用one-shot的方法降低参数量和训练时间

# 1.1 one-shot NAS方法的形式：将网络搜索问题解耦成train supernet 和 architecture search两部分， a 和 w 不会再有耦合关系，
      同时在train supernet的过程中使用DropPath的方法抑制 [公式] 之间过度地耦合。在Architecture search阶段随机sample子网络，
      权重使用supernet的权重做evaluation。当然也可以不随机采样，SPOS这个工作就用了进化算法)。
      
      但是，如果train supernet 的时候只是单纯地包含所有路径就会 leads to GPU memory explosion(这里说的就是Darts) , 
      as the memory consumption grows linearly w.r.t. the number of choices。ProxyLessNAS解决了这个问题。
      
      ProxyLessNAS train好了supernet之后，从over-parameterized network(supernet)开始，prune the redundant parts to derive
      the optimized architecture。你注意他这里从supernet到最终网络的过程貌似不是sample出来的，而是prune出来的，而且paper还说了
      与以往prune的区别：过去的prune是layer-level的，即topology不变。但是ProxyLessNAS是path-level的，目的是从supernet剪到最终网络。
      
      
# 1.2 优势：
      不需要使用代理任务，直接在大规模的数据集上搜索整个网络，而且直接学习所有的block
      为NAS提供了一种新的路径剪枝方式，展示了NAS和模型压缩之间的紧密关系；
      提出了一种基于梯度的方法来约束硬件指标；

#1.3 回答问题：
    proxyLessNAS在train supernet中如何直接在大规模数据集上训练：
       训练路径二值化，
    
    train好了supernet以后如何prune冗余的路径：
    
    https://zhuanlan.zhihu.com/p/144318917
    
进化算法：


强化学习：
      


      
